**Deep Learning Based Apparent Diffusion Coefficient Map Generation from Multi-parametric MR Images for Patients with Diffuse Gliomas**

Zach Eidex1, Mojtaba Safari1, Jacob Wynne1, Richard L.J. Qiu1, Tonghe Wang3, David Viar Hernandez1, Hui-Kuo Shu1,5, Hui Mao4,5 and Xiaofeng Yang1,2,5\*

1Department of Radiation Oncology, Emory University, Atlanta, GA

2School of Mechanical Engineering, Georgia Institute ofTechnology, Atlanta, GA 3Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY 4Department of Radiology and Imaging Sciences, Emory University, Atlanta, GA

5Winship Cancer Institute, Emory University, Atlanta, GA

**Running title:** ADC Map Synthesis **Manuscript Type:** Original Research

**Contact information:**

Email **–[ xiaofeng.yang@emory.edu ](mailto:xiaofeng.yang@emory.edu)**

Address - 1365-C Clifton Road NE Atlanta, Georgia 30322\*\*

**ABSTRACT**

**Purpose:** Apparent diffusion coefficient (ADC) maps derived from diffusion weighted magnetic resonance imaging (DWI MRI) provides functional measurements about the water molecules in tissues. However, DWI is time consuming and very susceptible to image artifacts, leading to inaccurate ADC measurements. This study aims to develop a deep learning framework to synthesize ADC maps from multi-parametric MR images.

**Methods:** We proposed the multiparametric residual vision transformer model (MPR-ViT) that leverages the long-range context of vision transformer (ViT) layers along with the precision of convolutional operators. Residual blocks throughout the network significantly increasing the representational power of the model. The MPR-ViT model was applied to T1w and T2- fluid attenuated inversion recovery images of 501 glioma cases from a publicly available dataset including preprocessed ADC maps. Selected patients were divided into training (N=400), validation (N=50) and test (N=51) sets, respectively. Using the preprocessed ADC maps as ground truth, model performance was evaluated and compared against the Vision Convolutional Transformer (VCT) and residual vision transformer (ResViT) models with the peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), and mean squared error (MSE).

**Results:** The results are as follows using T1w + T2-FLAIR MRI as inputs: MPR-ViT - PSNR: 31.0 ± 2.1, MSE: 0.009 ± 0.0005, SSIM: 0.950 ± 0.015. In addition, ablation studies showed the relative impact on performance of each input sequence. Both qualitative and quantitative results indicate that the proposed MR- ViT model performs favorably against the ground truth data.

**Conclusion:** We show that high-quality ADC maps can be synthesized from structural MRI using a MPR- VCT model. Our predicted images show better conformality to the ground truth volume than ResViT and VCT predictions. These high-quality synthetic ADC maps would be particularly useful for disease diagnosis and intervention, especially when ADC maps have artifacts or are unavailable.

**Keywords**: Glioma, DWI, intramodal MRI synthesis, deep learning, MRI

**INTRODUCTION**

Gliomas, graded in severity from I to IV by the World Health Organization (WHO), represent over half of malignant tumors that affect the central nervous system[.1 ](#_page11_x60.00_y98.00)Low grade gliomas (LGGs; WHO grades I and II) are less aggressive and offer a significantly improved prognosis compared to high grade gliomas (HGGs; WHO grades III and IV). The WHO grade is determined, along with molecular and genomic biomarkers by visualizing the tumor progression through medical imagi[ng.2 ](#_page11_x60.00_y124.00) Given magnetic resonance imaging’s (MRI) excellent soft tissue contrast, MRI has become a modality of choice to diagnose and prognosis for diffuse gliom[a.3 ](#_page11_x60.00_y149.00)MRI sequences including T1-weighted (T1w), T1-weighted postcontrast (T1c), T2-weighted (T2w), and T2 fluid attenuated inversion recovery (FLAIR) MRI provide precise tumor localization and structural information like peritumoral edema, necrosis, and the mass effect[.4 ](#_page11_x60.00_y187.00)However, tumor differentiation can still be a challenge with structural MRI since glioma grades can have similar appearances, so advanced techniques that capture the functional information of the tumor are often needed.

Diffusion weighted MRI (DWI) measures changes in cellular water mobility. Since tumors are typically marked by an increase in cellularity and, therefore, reduced water mobility, these differences in water diffusivity can be quantified by taking multiple DWIs at differing gradient strengths to create an apparent diffusion coefficient (ADC) map. DWI and ADC maps have demonstrated promise in determining glioma prognosis[.5 ](#_page11_x60.00_y212.00) However, DWI and ADC are highly susceptible to image artifacts due to the fast imaging acquisition techniques that are inherently susceptible to being corrupted by image artifacts[.6 ](#_page11_x60.00_y250.00)To address this issue, previous studies used deep learning models to generate ADC maps from under-sampled DWI to reduce acquisition time and the likelihood of artifact[s.](#_page11_x60.00_y275.00)[7,8 ](#_page11_x60.00_y301.00)Another approach removes the need for DWI altogether by leveraging structural MRI to predict the ADC ma[p.9](#_page11_x60.00_y339.00)[,10 ](#_page11_x60.00_y377.00)Unique to this study, we find that using multiple MR sequences (T1w and T2-FLAIR) to generate the synthetic ADC map volume allows for higher quality predictions than a single sequence alone.

While there are relatively few studies specific to ADC map synthesis, Intramodal MRI synthesis has seen significant progress alongside advances in advancements in natural image translation techniques. Most approaches use Pix2pix which claims to produce more realistic results by using generative adversarial network (GAN) with U-Net as a generator[.11](#_page11_x60.00_y402.00)[,12 ](#_page11_x60.00_y415.00)While Pix2pix can generate reasonable predictions, Pix2pix is exclusively based on convolutional layers so can struggle to capture long-range context. Recent approaches implement vision transformers (ViT) that capture long-range context by finding relationships throughout the entire feature map.[13](#_page11_x60.00_y453.00)[,14 ](#_page11_x60.00_y478.00) To mitigate the computational burden of vision transformers, hybrid CNN- transformers strategically place ViT layers in deeper, more abstract layers where feature map sizes are smaller and the attention calculation is less costly. The residual vision transformer (ResViT) model incorporates both vision transformers and a GAN architecture and supports multimodal inputs[.15 ](#_page11_x60.00_y503.00) However, the GAN architecture was found to introduce considerable training complexity and was not helpful in generating high- quality ADC maps. In addition, ResViT called for initially pretraining the convolutional layers first without transformer layers and pretraining the transformer layers on natural images but were not found to improve performance. By using only the ResViT generator and implementing several efficiency improvements like flash attention and the ability to natively use any resolution with dimensions divisible by eight, the vision convolutional neural network transformer (VCT) model is proposed for structural MRI to ADC map synthesis[.16-18](#_page11_x60.00_y541.00)

In this study, we build off the VCT model and propose the multiparametric residual vision transformer (MPR- ViT) model to produce highly accurate ADC maps. We make the following contributions:

1. This is the first attempt to produce synthetic ADC maps for patients with diffuse glioma or tumor volumes. We evaluate the model’s performance on heterogeneous regions compared to other state-of- the-art methods.
1. Leveraging efficiency improvements made in the VCT model, we are more easily able to significantly increase the VCT model’s representational power by replacing each encoder and decoder convolutional block with 3 residual blocks.
1. We achieve state-of-the-art performance for single-modal (T1w → ADC map and T2-FLAIR → ADC map) along with multimodal (T1w + T2-FLAIR → ADC map) image synthesis with the MPR- ViT model.

1) **METHOD**
1) **Data Acquisition and Preprocessing**

Patients were selected from the publicly available The Cancer Imaging Archive (TCIA) University of California San Francisco Preoperative Diffuse Glioma (UCSF-PDGM) containing 501 adult patients with T1w and T2-FLAIR MRI along with ADC map[s.19](#_page11_x60.00_y617.00)[,20 ](#_page11_x60.00_y655.00)All scans were performed on a 3.0T scanner (Discovery 750, GE Healthcare, Waukesha, Wisconsin, USA) and dedicated 8-channel head coil (Invivo, Gainesville, Florida, USA) between 2015 and 2021. The patients all had histopathologically confirmed grade II-IV diffuse gliomas including 55 (11%) grade II, 42 (9%) grade III, and 403 (80%) grade IV tumors. Segmentation maps were created from an ensemble model using top ranking segmentation algorithms for the whole brain volume as well as the three major tumor regions: enhancing tumor, necrotic tumor, and peritumoral edem[a.21 ](#_page12_x60.00_y72.00)These segmentation maps were then corrected by trained radiologists and approved by 2 expert reviewers. The ADC maps were produced from 2D 55-direction high angular resolution diffusion imaging and were corrected for Eddy currents. These DWI were then registered and resampled to the T2-FLAIR image at 1 mm isotropic resolution through automated non-linear registration, and skull stripped with a publicly available deep- learning algorithm[.22](#_page12_x60.00_y98.00)[,23 ](#_page12_x60.00_y136.00)These images were then transformed into ADC maps. All volumes downsampled by a factor of 2 using bicubic interpolation. The down-sampling was done to reduce computational burden.

2. **MPR-ViT Architecture**

Figure 1 illustrates the proposed architecture, which T1-weighted (T1w) and 3T T2-FLAIR axial slices are concatenated channel-wise to produce synthetic ADC maps. The encoder and decoder learn localized features through convolutional layers connected with residual skip connections while the information bottleneck encourages learning more abstract, long-range features though residual convolutional and transformer layers. The encoder and decoder each consist of 3 combined residual blocks defined to be three sequential residual blocks of the same dimensionality. Compared with single residual blocks, the combined residual blocks allow for increased abstraction and representational power. The information bottleneck similarly allows for a high degree of abstraction by incorporating 11 layers connected with residual skip connections and does not change the dimensions of the feature map. Two of these layers incorporate transformers to allow for long- range context and share weights to decrease the computational burden. In total, MPR-ViT contains 27 residual blocks and 2 ViT blocks.

![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.001.jpeg)

**Figure 1.** (a) Schematic flow chart of the MPR-ViT model. The green, orange, and blue blocks are combined residual, ViT, and convolutional blocks, respectively. Curled arrows represent residual skip connections bypassing the in-between layer to facilitate training deeper networks. (b) Each combined residual block shown in (a) corresponds to 3 residual blocks. These additional layers increase the depth and representational capacity of the model. (c) The feature maps are downsampled with convolutional blocks, flattened, and coupled with position embeddings before being passed into the transformer encoder. After upsampling, a residual skip connection prevents the loss of previously derived context. Finally, the channel compression module better reduces the number of feature maps than simple downsampling. (d) A standard transformer encoder with the inclusion of the flash attention mechanism.

3. **Encoder and Decoder**

The encoder and decoder are each composed of 3 combined residual blocks. Each combined residual block contains 3 residual blocks containing a convolutional layer followed by a norm layer and rectified linear unit (ReLU) activation function. All convolutional layers have a kernel size of 3×3 except the very first and last layers which have a large kernel size of 7×7 to capture broader the receptive field. Each convolutional block has a residual skip connection to facilitate the deeper network by backpropagating errors. The final convolutional layer of the combined residual block in the encoder has a stride of 2 to reduce the dimensionality by a factor of 2 while the combined residual block of the decoder is a transposed convolution which doubles the feature map size. In total the dimensionality is reduced by a factor of 4 to a resolution of 30×30 before being passed into the information bottleneck and ultimately returned to the original input size of 120x120 in encoder. This encourages the information bottleneck to focus on course details and reduces the computational burden of the ViT blocks. The encoder and decoder are symmetric except that the final convolutional block in the decoder sets the output number of channels to 1 compared to the 2 input channels in the encoder. In addition, the final layer in the network was followed by a hyperbolic tangent activation function to ensure the range of the output feature map values is from -1 to 1.

4. **Information Bottleneck**

The information bottleneck captures abstract, global context using both convolutional blocks and powertful but computationally expensive ViT blocks. Residual skip connections are placed across both blocks to mitigate the vanishing gradient problem by creating an alternate, shorter path for the gradient during backpropagation.[24 ](#_page12_x60.00_y186.00)The convolutional blocks in the information bottleneck shown in Figure 1 are comprised of two sequential 3 × 3 convolutional layers which are then connected to the input feature map through a skip connection. The ViT blocks similarly incorporate two sequential 3 × 3 convolutional layers but use a stride of

2 which reduces the dimensionality of the input feature maps by a factor of 4 to ease the computational burden. Afterwards, the feature maps are flattened and given each feature on the feature map is given before being input into the transformer layer. After the transformer layer, two 3 × 3 transposed convolutional layers return the feature maps to their original spatial dimensions before passing through the ViT block. Finally, this output is connected to the input feature map through a residual skip connection. Since the ViT block downsamples the feature map from 30×30 by a factor of 4 rounding to 8×8, then upsampling to 32x32, we implement bilinear interpolation to resize it back to 30x30. We chose this approach in favor of padding the input images by 128×128 to improve efficiency and because we did not notice a decline in performance. Finally, we replace the traditional attention mechanism with more efficient FlashAttenti[on.25 ](#_page12_x60.00_y199.00)Read-and-write operations to GPU memory are the primary bottleneck for the attention mechanism's calculation speed in transformers. Flash Attention addresses this issue by optimizing the use of the GPU's small, high-performance SRAM cache through tiling and recomputation. This approach lessens the memory demand while also achieving a notable increase in speed and remains computationally equivalent to the traditional attention mechanism.

5. **Implementation Details**

The MPR-ViT model was trained on a consumer-grade NVIDIA RTX 4090 GPU with 24 GB of memory, and additional results were gathered with a cloud-based NVIDIA A10 with 24 GB of memory. The dataset was augmented by randomly flipping the images in the coronal plane. An AdamW gradient optimizer (learning rate 2e-4, β1 = .500, β2 = .999, eps = 1e-6) was set to optimize the learnable parameters over 251 epochs or when the model no longer reduced the validation loss. For the multimodal hold-out test, training was stopped at 171 epochs. The AdamW optimizer was chosen to minimize the loss function (L1 loss) for its improved generalization performance over the Adam optimizer due to a decoupling of the weight decay and gradient updat[e.26 ](#_page12_x60.00_y224.00)32 image slices were used for each batch. Each epoch took approximately 4 minutes, and the inference time of each image slice during testing was 3.4 milliseconds (ms) or 238 ms for a typical patient volume with 70 slices.

6. **Validation and Evaluation**

Model performance was assessed using a hold-out test by randomly dividing a total of 501 patients into training (400 patients: 29,005 slices), validation (50 patients: 3,511 slices), and testing (51 patients: 3,590 slices) slices. Results were quantified using mean squared error (MSE), peak signal-to-noise ratio (PSNR), and structural similarity index (SSIM) over the entire 3D volume. Student’s two-sided t-test was employed to compare the VCT model results with comparative methods. The significance level was set at 0.05 for these evaluation metrics. MSE measures the voxel wise difference between the synthetic and ground truth volumes such that a value of zero means no difference[.27 ](#_page12_x60.00_y237.00)PSNR is inversely related with the MSE so that higher PSNR values correspond to higher similarity to the ground truth volume. Logarithmic scaling was applied to make it more closely align with human perceptio[n.28 ](#_page12_x60.00_y262.00)SSIM considers luminance, contrast, and structural similarity functions to most closely align with human perception. SSIM values range from -1 to 1 with 1 being perfect correspondence with the ground truth volum[e.29 ](#_page12_x60.00_y300.00)MSE, SSIM, and PSNR are defined below _where n_ is the total number of voxels, _Xi_ and _Yi_ are the voxel intensity of the synthetic and ground truth volumes, and _MAXI_ is the maximum possible voxel value of the ground truth volumes.\*\*

- 1 ∑ ( − )2 (1) =1
- 10 ( 2) (2) ![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.002.png)

(_2μ μ_ + 1)(_2σ_ + 2)

- _(x,y) ⋅ c(x,y) ⋅ **s**(x,y) =_ (3)* (*μ*2+*μ*2 + 1)(*σ*2+*σ\*2+ 2)

The SSIM is comprised of luminescence (_l(x,y))_, contrast (_c(x,y)_), and structural similarity (**\*s**(x,y))* functions. *μ* and *μy\* are the means of the synthetic and ground truth volumes, σ 2, σy2, and σ are the variance of the

_x_ x xy

synthetic volume, the variance of the ground truth volume, and the covariance between the synthetic and ground truth ADC maps respectively. C1 and C2 are small constants to avoid division by zero; 1 = 0.01 and

2 = 0.02 where L osthe maximum value of the target.

To assess the complexity of the model, the number of parameters and floating-point operations per second (FLOPS) of the model were calculated.

2. **RESULTS**

Synthetic ADC maps generated by the MPR-ViT model are compared against the ground truth ADC and evaluated through the MSE, PSNR, and SSIM metrics (see Table 1). Compared with the ResViT and VCT architectures, MPR-ViT showed a statistically significant improvement (p-value < .001) using both unimodal and multimodal inputs achieving an MSE: 0.0009 ± 0.0005, PSNR: 30.9 ± 2.0, SSIM:0.950 ± 0.015. Both VCT and MPR-ViT benefited from a multimodal approach while ResViT achieved its highest performance with T2-FLAIR only inputs. Example cases of output images from 4 patients are shown in Figure 2 with zoom-in on regions along with ground truth ADC maps and T1w and T2-FLAIR MRI input images. Figure 2a-c show patients with the tumor volumes while Figure 2d shows a tumor-free region. VCT most closely resembles the ground truth (GT) ADC maps while ResViT produces the worst outputs.

Configurations of the VCT model with increasing numbers of residual blocks added to the combined residual blocks are evaluated in Table 2. The VCT model saw marked improvements in the SSIM value with the addition of one and two residual blocks but gains in performance after 3 layers. Model complexity rose predictably in terms of model size and computational cost with additional residual blocks leading to longer training times.

**Table 1**. Quantitative results for the whole brain volume. P-value metrics are compared against MPR-ViT’s performance. The arrows indicate the direction of the better metric value.

**MSE[↓] PSNR[↑] SSIM[↑] P-Value ![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.003.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.004.png)**

**T1w Only ![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.005.png)**

T1w MRI .0105 ± .0036 20.0 ± 1.5 .692 ± .040 <.001 ![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.006.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.007.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.008.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.009.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.010.png)ResViT .0014 ± .0005 28.8 ± 1.5 .925 ± .019 <.001 ![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.011.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.012.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.013.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.014.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.015.png)VCT .0012 ± .0004 29.6 ± 1.6 .934 ± .017 <.001 ![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.016.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.017.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.018.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.019.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.020.png)

MPR-ViT **.0011 ± .0005 30.1 ± 1.9 .939 ± .017** X ![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.021.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.022.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.023.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.024.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.025.png)**FLAIR Only ![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.026.png)**

FLAIR MRI .0144 ± .0091 19.4 ± 3.0 .768 ± .034 <.001 ![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.027.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.028.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.029.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.030.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.031.png)![ref1] ResViT ![ref2] .0013 ± .0005 ![ref3] 29.1 ± 1.5 ![ref4] .932 ± .016 ![ref5] <.001 VCT .0022 ± .0008 26.9 ± 1.5 .889 ± .025 <.001 ![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.037.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.038.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.039.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.040.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.041.png)

MPR-ViT **.0010 ± .0005 30.4 ± 1.9 .945 ± .015** X ![ref3]![ref5]![ref4]![ref1]![ref2]**T1w + FLAIR ![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.042.png)**

ResViT .0014 ± .0005 28.9 ± 1.5 .927 ± .017 <.001 ![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.043.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.044.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.045.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.046.png)![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.047.png)![ref1] VCT ![ref2] .0010 ± .0006 ![ref3] 30.4 ± 2.1 ![ref4] .945 ± .016 ![ref5] <.001 MPR-ViT **.0009 ± .0005 31.0 ± 2.1 .950 ± .015** X ![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.048.png)

![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.049.jpeg)

**Figure 2.** Example ADC maps generated from ResViT, VCT, and MPR-ViT from 4 patients along with zoom-in on interesting regions. All methods took both T1w and T2-FLAIR MRI as inputs. (a) - (d) show reconstructions of slices containing tumor. Shown in the segmentation map in order from darkest to lightest are the necrotic tumor core, brain mask, peritumoral edema, and enhancing tumor.

**Table 2.** Model Size and Performance. The VCT model improved by adding additional residual blocks to each combined ![](Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.050.png)residual block up to 2 residual blocks (RB). Since there are 6 combined ~~residual blocks~~RBs in the network, VCT + 1 RB corresponds to adding 6 RB total. All models were trained on T1w + T2-FLAIR and with a batch size of 32.

|                        | **Model Size** **(Millions of Parameters)** | **FLOPS (Billions)** | **SSIM**    |
| :--------------------- | :------------------------------------------ | -------------------- | ----------- |
| ResViT (Generator)     | 123                                         | 1378                 | .927 ± .017 |
| ResViT (Discriminator) | 4                                           | 33                   | X           |
| VCT                    | 115                                         | 877                  | .945 ± .016 |
| VCT + 1 RB             | 117                                         | 1068                 | .949 ± .016 |
| VCT + 2 RB (MPR-ViT)   | 119                                         | 1277                 | .951 ± .016 |
| VCT + 3 RB             | 121                                         | 1486                 | .948 ± .015 |
| VCT + 4 RB             | 123                                         | 1694                 | .950 ± .015 |

3. **DISCUSSION**

In this study, we propose the MPR-ViT model for multimodal ADC map synthesis which outperforms the ResViT and VCT models as measured by MSE, PSNR, and SSIM and based on qualitative results. By generating accurate ADC maps from multi-modal, structural MRI, greater contrast and detail can be achieved when DWI contains artifacts or is unavailable. The synthetic ADC maps generated by the MPR-ViT model are, overall, highly conformal to the ground truth volume even in difficult heterogeneous and tumor regions. These favorable image characteristic may have an increased ability for tumor characterization and detection so synthetic ADC maps may prove to be clinically useful by demonstrating superior clinical information.[30](#_page12_x60.00_y338.00)[,31](#_page12_x60.00_y376.00)

To our knowledge, this is the first work to synthesize ADC maps from multimodal MRI. However, several notable studies have been published for multi-modal and ADC map MR image translation tasks[.32 ](#_page12_x60.00_y414.00)Modifying the StarGAN architecture for intramodal MRI synthesis, Dai et al generate four MR sequences (T1w, T2w, T1-postcontrast (T1c), and T2 Fluid attenuated inversion recovery (FLAIR)) simultaneously from a single input sequence. Using T1w MRI as input, the SSIM for T1c, T2w, and T2-FLAIR were 0.974±0.059, 0.969±0.059, and 0.959±0.059 respectively[.33](#_page12_x60.00_y439.00)[,34 ](#_page12_x60.00_y465.00) Wang et al incorporated unsupervised and supervised learning to synthesize ADC maps from T2w MRI with the ultimate goal of localizing prostate cancer lesions, reporting a Fréchet inception distance score of 178.2 ± 3.7.[35](#_page12_x60.00_y490.00)[,36](#_page12_x60.00_y515.00)

Comparing the input T1w and T2-FLAIR MRI with the synthetic ADC maps, this study reveals that the synthetic ADC maps provide a markedly higher contrast and reveal additional detail, especially of the tumor volume for instance in Figure 2A. In addition, Table 1 demonstrates the usefulness of a multi-modal approach to intramodal MR image synthesis where VCT and MPR-ViT perform the best with T1w + T2-FLAIR MRI as input modalities. We believe this is because the two modalities together provide a richer feature space than a single sequence especially since the peritumoral edema appears as a hyperintense signal on T2-FLAIR[.37](#_page12_x60.00_y541.00)

However, ResViT performed better using only T2-FLAIR as input. In our previous work, we explored the process of generating7T ADC maps from 3T ADC maps + T1w MRI, we observed instances where ResViT became confused by the T1w MRI.We speculate that the same behavior might be at play in our current work as well[.18 ](#_page11_x60.00_y592.00)Shown in Figure 2, the synthetic ADC maps generated by the MPR-ViT model are highly similar to the ground truth ADC maps. Moreover, the MPR-ViT model demonstrated the ability to recover detail only visible in ground truth ADC map with especially impressive results in Figure 2a. However, we also note that the synthetic ADC maps are imperfect, especially in heterogeneous regions. This is best shown in the zoomed in region of Figure 2b in which the original tumor volume contains necrosis beneath an edema region The MPR-ViT model achieved the most similar results in this example but still did not capture the necrosis region although it can be seen in the T1w scan. In addition, we note that the ResViT model appears to generate images with less blurring than MPR-ViT, but also adds extraneous details that are not true to the ground truth ADC maps. We believe this is a direct result of ResViT’s GAN architecture designed to make images appear more realistic at the cost of giving less importance to pixel-wise accuracy. As verified quantitatively in Table 1, the MPR-ViT model outperforms ResViT and VCT, establishing a new state-of-the-art in all evaluation metrics (MSE, PSNR, and SSIM).

The primary difference between the VCT and MPR-ViT models is the addition of 2 residual blocks forming a combined residual block. As a model is made deeper, it is intuitive that the representational power should increase leading to better performance. For example, the error rate decreases with additional layers in ResNet models with ResNet-152 showing significantly better performance than ResNet-[50.38 ](#_page12_x60.00_y566.00)However, shown in Table 2, the VCT model did not show additional improvement after 2 additional residual blocks. A potential reason for this is that larger models are more difficult to train and require more data to benefit from the additional parameters. In terms of computational cost, MPR-ViT required approximately 4 million additional parameters and 46% more FLOPS than VCT, but this was still manageable and less than ResViT because of the earlier improvements in the VCT model. Importantly, MPR-ViT was still capable of running on 24 GB of memory with a batch size of 32.

We acknowledge several limitations of the present work. The MPR-ViT model presented here is trained on 2D slices and so does not directly capture the full 3D context of the input data. While the convolutional layers employed in VCT provide an efficient way to capture the fine details, transformers could be added to the encoder and decoder although at a high computational cost due to the larger feature map sizes. Also, from our ablation studies adding residual blocks, we note that increased model complexity can degrade performance, so any additional transformer layers would need to be added carefully. The dataset was comprised of only glioma patients, so it remains to be seen how the results will generalize to patients with other diseases. However, given the successful application of related image translation tasks in these settings, we are optimistic about the generalizability of this model[.39-41 ](#_page12_x60.00_y578.00)We intend in future work to incorporate full 3D context, train on larger more diverse datasets as they become availabl[e42](#_page13_x60.00_y72.00)[,43,](#_page13_x60.00_y98.00) explore diffusion model[s44,](#_page13_x60.00_y136.00) and see if these synthetic ADC maps can be used to differentiate LGG from HGG. In addition, we would also like to see if including under-sampled DWI as an input sequence, in addition to structural MRI, further improves the ADC map predictions.

**5. CONCLUSION**

This study presents a deep residual hybrid CNN-transformer model designed for multimodal ADC map synthesis. Our model achieved marked improvements in accuracy compared to current state-of-the art methods were achieved while still being efficient enough to be practical. The proposed method shows great promise in making valuable information in ADC maps more readily available.

**ACKNOWLEDGMENTS**

This research is supported in part by the National Cancer Institute of the National Institutes of Health under Award Numbers R01CA272991, R56EB033332 and P30 CA008748. This work was supported in part by Oracle Cloud credits and related resources provided by Oracle for Research.

**Disclosures**

The author declares no conflicts of interest.

**Reference:**

1. Miller<a name="_page11_x60.00_y98.00"></a> KD, Ostrom QT, Kruchko C, et al. Brain and other central nervous system tumor statistics, 2021. _CA: A Cancer Journal for Clinicians._ 2021;71(5):381-406.
1. Louis<a name="_page11_x60.00_y124.00"></a> DN, Perry A, Wesseling P, et al. The 2021 WHO Classification of Tumors of the Central Nervous System: a summary. _Neuro Oncol._ 2021;23(8):1231-1251.
1. Verburg<a name="_page11_x60.00_y149.00"></a> N, Hoefnagels FWA, Barkhof F, et al. Diagnostic Accuracy of Neuroimaging to Delineate Diffuse Gliomas within the Brain: A Meta-Analysis [published online ahead of print 20170907]. _AJNR Am J Neuroradiol._ 2017;38(10):1884-1891.
1. Shukla<a name="_page11_x60.00_y187.00"></a> G, Alexander GS, Bakas S, et al. Advanced magnetic resonance imaging in glioblastoma: a review. _Chin Clin Oncol._ 2017;6(4):40.
1. Lawrence<a name="_page11_x60.00_y212.00"></a> LSP, Chan RW, Chen H, et al. Diffusion-weighted imaging on an MRI-linear accelerator to identify adversely prognostic tumour regions in glioblastoma during chemoradiation [published online ahead of print 20230826]. _Radiother Oncol._ 2023;188:109873.
1. Le<a name="_page11_x60.00_y250.00"></a> Bihan D, Poupon C, Amadon A, Lethimonnier F. Artifacts and pitfalls in diffusion MRI. _Journal <a name="_page11_x60.00_y275.00"></a>of Magnetic Resonance Imaging._ 2006;24(3):478-488.
1. Aamir F, Aslam I, Arshad M, Omer H. Accelerated Diffusion-Weighted MR Image Reconstruction Using Deep Neural Networks. _Journal of Digital Imaging._ 2023;36(1):276-288.
1. Li<a name="_page11_x60.00_y301.00"></a> Y, Joaquim MR, Pickup S, Song HK, Zhou R, Fan Y. Learning ADC maps from accelerated radial k-space diffusion-weighted MRI in mice using a deep CNN-transformer model [published online ahead of print 20230820]. _Magn Reson Med._ 2024;91(1):105-117.
1. Wang<a name="_page11_x60.00_y339.00"></a> Z, Lin Y, Cheng KT, Yang X. Semi-supervised mp-MRI data synthesis with StitchLayer and auxiliary distance maximization [published online ahead of print 20191001]. _Med Image Anal._ 2020;59:101565.
1. Chan<a name="_page11_x60.00_y377.00"></a> K, Maralani PJ, Moody AR, Khademi A. Synthesis of diffusion-weighted MRI scalar maps from FLAIR volumes using generative adversarial networks. _Frontiers in Neuroinformatics._ 2023;17.
1. Goodfellow<a name="_page11_x60.00_y402.00"></a> IJ, Pouget-Abadie J, Mirza M, et al. _Generative Adversarial Networks._
1. Isola<a name="_page11_x60.00_y415.00"></a> P, Zhu JY, Zhou T, Efros AA. Image-to-Image Translation with Conditional Adversarial Networks. Paper presented at: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); 21-26 July 2017, 2017.
1. Alzubaidi<a name="_page11_x60.00_y453.00"></a> L, Zhang J, Humaidi AJ, et al. Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. _Journal of Big Data._ 2021;8(1):53.
1. Khan<a name="_page11_x60.00_y478.00"></a> RF, Lee BD, Lee MS. Transformers in medical image segmentation: a narrative review [published online ahead of print 20231007]. _Quant Imaging Med Surg._ 2023;13(12):8747-8767.
1. Elmi<a name="_page11_x60.00_y503.00"></a> S, Morris B. Res-ViT: Residual Vision Transformers for Image Recognition Tasks. Paper presented at: 2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI); 6-8 Nov. 2023, 2023.
1. Dao<a name="_page11_x60.00_y541.00"></a> T, Fu DY, Ermon S, Rudra A, Ré C. _FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awar eness._
1. Okada T, Fujimoto K, Fushimi Y, et al. Neuroimaging at 7 Tesla: a pictorial narrative review. _Quant Imaging Med Surg._ 2022;12(6):3406-3435.
1. Eidex<a name="_page11_x60.00_y592.00"></a> Z, Wang J, Safari M, et al. _High-resolution 3T to 7T MRI Synthesis with a Hybrid CNN- Transformer M odel._
1. Calabrese<a name="_page11_x60.00_y617.00"></a> E, Villanueva-Meyer JE, Rudie JD, et al. The University of California San Francisco Preoperative Diffuse Glioma MRI Dataset [published online ahead of print 20221005]. _Radiol Artif Intell._ 2022;4(6):e220058.
1. Clark<a name="_page11_x60.00_y655.00"></a> K, Vendt B, Smith K, et al. The Cancer Imaging Archive (TCIA): maintaining and operating a public information repository. _J Digit Imaging._ 2013;26(6):1045-1057.
1. Bakas<a name="_page12_x60.00_y72.00"></a> S, Reyes M, Jakab A, et al. _Identifying the Best Machine Learning Algorithms for Brain Tumor Segme ntation, Progression Assessment, and Overall Survival Prediction in th e BRATS Challenge._
1. Calabrese<a name="_page12_x60.00_y98.00"></a> E, Villanueva-Meyer JE, Cha S. A fully automated artificial intelligence method for non- invasive, imaging-based identification of genetic alterations in glioblastomas. _Scientific Reports._ 2020;10(1):11852.
1. Calabrese<a name="_page12_x60.00_y136.00"></a> E, Rudie JD, Rauschecker AM, Villanueva-Meyer JE, Cha S. Feasibility of Simulated Postcontrast MRI of Glioblastomas and Lower-Grade Gliomas by Using Three-dimensional Fully Convolutional Neural Networks [published online ahead of print 20210519]. _Radiol Artif Intell._ 2021;3(5):e200276.
1. He<a name="_page12_x60.00_y186.00"></a> K, Zhang X, Ren S, Sun J. _Deep Residual Learning for Image Recognition._
1. Dao<a name="_page12_x60.00_y199.00"></a> T, Fu DY, Ermon S, Rudra A, Ré C. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. _arXiv [csLG]._ 2022.
1. Loshchilov<a name="_page12_x60.00_y224.00"></a> I, Hutter F. _Decoupled Weight Decay Regularization._
1. Wang<a name="_page12_x60.00_y237.00"></a> J, Sohn JJ, Lei Y, et al. Deep learning-based protoacoustic signal denoising for proton range verification [published online ahead of print 20230512]. _Biomed Phys Eng Express._ 2023;9(4).
1. Peng<a name="_page12_x60.00_y262.00"></a> J, Qiu RLJ, Wynne JF, et al. CBCT-Based synthetic CT image generation using conditional denoising diffusion probabilistic model [published online ahead of print 20230830]. _Med Phys._ 2023. doi: 10.1002/mp.16704.
1. Xie<a name="_page12_x60.00_y300.00"></a> H, Lei Y, Wang T, et al. Magnetic resonance imaging contrast enhancement synthesis using cascade networks with local supervision [published online ahead of print 20220307]. _Med Phys._ 2022;49(5):3278-3287.
1. Unsgård<a name="_page12_x60.00_y338.00"></a> RG, Doan TP, Nordlid KK, Kvistad KA, Goa PE, Berntsen EM. Transient global amnesia: 7 Tesla MRI reveals more hippocampal lesions with diffusion restriction compared to 1.5 and 3 Tesla MRI [published online ahead of print 20220627]. _Neuroradiology._ 2022;64(12):2217-2226.
1. Rutland<a name="_page12_x60.00_y376.00"></a> JW, Loewenstern J, Ranti D, et al. Analysis of 7-tesla diffusion-weighted imaging in the prediction of pituitary macroadenoma consistency [published online ahead of print 20200228]. _J Neurosurg._ 2020;134(3):771-779.
1. Eidex<a name="_page12_x60.00_y414.00"></a> Z, Ding Y, Wang J, et al. Deep Learning in MRI-guided Radiation Therapy: A Systematic Review [published online ahead of print 20230330]. _ArXiv._ 2023.
1. Choi<a name="_page12_x60.00_y439.00"></a> Y, Choi M, Kim M, Ha J-W, Kim S, Choo J. _StarGAN: Unified Generative Adversarial Networks for Multi-Domain Imag e-to-Image Translation._
1. Dai<a name="_page12_x60.00_y465.00"></a> X, Lei Y, Fu Y, et al. Multimodal MRI synthesis using unified generative adversarial networks [published online ahead of print 20201027]. _Med Phys._ 2020;47(12):6343-6354.
1. Wang<a name="_page12_x60.00_y490.00"></a> Z, Lin Y, Cheng K-T, Yang X. Semi-supervised mp-MRI data synthesis with StitchLayer and auxiliary distance maximization. _Medical Image Analysis._ 2020;59:101565.
1. Heusel<a name="_page12_x60.00_y515.00"></a> M, Ramsauer H, Unterthiner T, Nessler B, Hochreiter S. _GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium._
1. Bakas<a name="_page12_x60.00_y541.00"></a> S, Akbari H, Sotiras A, et al. Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features. _Scientific Data._ 2017;4(1):170117.
1. He<a name="_page12_x60.00_y566.00"></a><a name="_page12_x60.00_y578.00"></a> K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. _arXiv [csCV]._ 2015.
1. Lei Y, Harms J, Wang T, et al. MRI-only based synthetic CT generation using dense cycle consistent generative adversarial networks [published online ahead of print 20190612]. _Med Phys._ 2019;46(8):3565-3581.
1. Liu Y, Lei Y, Wang Y, et al. MRI-based treatment planning for proton radiotherapy: dosimetric validation of a deep learning-based liver synthetic CT generation method. _Physics in Medicine & Biology._ 2019;64(14):145015.
1. Dong X, Wang T, Lei Y, et al. Synthetic CT generation from non-attenuation corrected PET images for whole-body PET imaging. _Physics in Medicine & Biology._ 2019;64(21):215016.
1. Liu<a name="_page13_x60.00_y72.00"></a> Z, Lin Y, Cao Y, et al. _Swin Transformer: Hierarchical Vision Transformer using Shifted Window s._
1. Pan<a name="_page13_x60.00_y98.00"></a> S, Wang T, Qiu RLJ, et al. 2D medical image synthesis using transformer-based denoising diffusion probabilistic model [published online ahead of print 20230505]. _Phys Med Biol._ 2023;68(10).
1. Dhariwal<a name="_page13_x60.00_y136.00"></a> P, Nichol A. _Diffusion Models Beat GANs on Image Synthesis._
   14

[ref1]: Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.032.png
[ref2]: Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.033.png
[ref3]: Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.034.png
[ref4]: Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.035.png
[ref5]: Aspose.Words.0fb3970e-db6d-4f77-920c-b932b8e6d60d.036.png
